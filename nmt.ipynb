{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28602591-bd9f-467c-a015-11acf39a5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71e62732-00db-4b4e-8a3e-69be08de0fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58b39d2b-bbf2-4235-ad90-af8ee6d59ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea59b269-3071-4785-9651-fe811872aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params = {\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 1,\n",
    "\n",
    "    # Encoder parameters\n",
    "    \"encoder_embedding_size\": 300,\n",
    "    \"encoder_dropout\": 0.5,\n",
    "\n",
    "    # Decoder parameters\n",
    "    \"decoder_dropout\": 0.5,\n",
    "    \"decoder_embedding_size\": 300,\n",
    "\n",
    "    # Common parameters\n",
    "    \"hidden_size\": 1024,\n",
    "    \"num_layers\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77f80e9c-9731-4b1c-b4f3-639e93958911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        \n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "\n",
    "        return hidden_state, cell_state\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        # as we are not feeding whole sentence we will each token a time\n",
    "        # hence our sequence length would be just 1 however shape of x is batch_size\n",
    "        # to add sequence length we will unsequeeze it\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedded, (hidden_state, cell_state))\n",
    "\n",
    "        # Shape --> predictions  (1, batch_size, target_vocab_size)\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # Shape --> predictions (32, 4556) (batch_size , target_vocab_size)\n",
    "        predictions = self.softmax(predictions.squeeze(0))\n",
    "\n",
    "        return predictions, hidden_state, cell_state\n",
    "\n",
    "class SeqtoSeq(nn.Module):\n",
    "    def __init__(self, gen_params, target_vocab, device):\n",
    "        super(SeqtoSeq, self).__init__()\n",
    "\n",
    "        self.Encoder = Encoder(gen_params[\"input_size_encoder\"],\n",
    "                          gen_params[\"encoder_embedding_size\"],\n",
    "                          gen_params[\"hidden_size\"],\n",
    "                          gen_params[\"num_layers\"],\n",
    "                          gen_params[\"encoder_dropout\"]).to(device)\n",
    "\n",
    "        self.Decoder = Decoder(gen_params[\"input_size_decoder\"],\n",
    "                          gen_params[\"decoder_embedding_size\"],\n",
    "                          gen_params[\"hidden_size\"],\n",
    "                          gen_params[\"num_layers\"],\n",
    "                          gen_params[\"decoder_dropout\"],\n",
    "                          gen_params[\"output_size\"]).to(device)\n",
    "\n",
    "        self.target_vocab = target_vocab\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, source, target, tfr=0.5):\n",
    "        # Shape -> (Sentence length, Batch_size)\n",
    "        batch_size = source.shape[1]\n",
    "\n",
    "        target_len = target.shape[0]  # Length of target sentences\n",
    "        target_vocab_size = len(self.target_vocab)\n",
    "        \n",
    "        # here we will store all the outputs\n",
    "        # so outputs is arrange in a way that sentences are in column and batch size is row and every element\n",
    "        # will consist of probability of each word from the vocab\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
    "\n",
    "        # Shape --> (hs, cs) (num_layers, batch_size size, hidden_size) (contains encoder's hs, cs - context vectors)\n",
    "        hidden_state, cell_state = self.Encoder(source)\n",
    "\n",
    "        # Shape of x (32 elements)\n",
    "        x = target[0]  # First token (Trigger)\n",
    "\n",
    "        for i in range(1, target_len):\n",
    "            # Shape --> output (batch_size, target_vocab_size)\n",
    "            output, hidden_state, cell_state = self.Decoder(x, hidden_state, cell_state)\n",
    "            outputs[i] = output\n",
    "            best_guess = output.argmax(1)  # 0th dimension is batch size, 1st dimension is word embedding\n",
    "            # Schedule sampling\n",
    "            x = target[\n",
    "                i] if random.random() < tfr else best_guess  # Either pass the next word correctly from the dataset\n",
    "            # or use the earlier predicted word\n",
    "\n",
    "        # Shape --> (sentence length, batch size, vocab size)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "772e5645-da1e-4ac6-9bd1-ab3e2ffb13f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maqbool/.virtualenvs/nmt/lib/python3.6/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n",
      "/Users/maqbool/.virtualenvs/nmt/lib/python3.6/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"de\" could not be loaded, trying \"de_core_news_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "class nmtDataset(Dataset):\n",
    "    def __init__(self, ds_path, split):\n",
    "        self.en = open(ds_path + split + '.en', encoding='utf-8').readlines()\n",
    "        self.de = open(ds_path + split + '.de', encoding='utf-8').readlines()\n",
    "        \n",
    "        self.tokenizers = { 'en': get_tokenizer('spacy', language='en'), \n",
    "                            'de': get_tokenizer('spacy', language='de') }\n",
    "        \n",
    "        self.src_vocab, self.trg_vocab = self._build_vocab()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.en)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        src_tokens = self.tokenizers['en'](self.en[item].strip())\n",
    "        trg_tokens = self.tokenizers['de'](self.de[item].strip())\n",
    "  \n",
    "        return {\n",
    "            \"src\": self.src_vocab.lookup_indices(src_tokens),\n",
    "            \"trg\": [self.trg_vocab['<sos>']] + self.trg_vocab.lookup_indices(trg_tokens) + [self.trg_vocab['<eos>']]\n",
    "        }\n",
    "        \n",
    "    def _build_vocab(self):\n",
    "        src_vocab = build_vocab_from_iterator(self._get_tokens(self.en, 'en'), specials=['<unk>', '<pad>', '<sos>', '<eos>'], min_freq=10)  # discarding words occurs < 10 times\n",
    "        trg_vocab = build_vocab_from_iterator(self._get_tokens(self.de, 'de'), specials=['<unk>', '<pad>', '<sos>', '<eos>'], min_freq=10)\n",
    "  \n",
    "        trg_vocab.set_default_index(trg_vocab['<unk>'])\n",
    "        src_vocab.set_default_index(src_vocab['<unk>'])\n",
    "\n",
    "        return src_vocab, trg_vocab\n",
    "    \n",
    "    def _get_tokens(self, corpus, lang):\n",
    "        for line in corpus:\n",
    "            yield self.tokenizers[lang](line.strip())\n",
    "    \n",
    "nmtds_train = nmtDataset('datasets/Multi30k/', 'train')\n",
    "nmtds_valid = nmtDataset('datasets/Multi30k/', 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73779a22-9baa-49d5-a298-13b1abe6d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_fn(batch, device):\n",
    "    PAD_IDX = 1\n",
    "    trgs = []\n",
    "    srcs = []\n",
    "    for row in batch:\n",
    "        srcs.append(torch.tensor(row[\"src\"]).to(device))\n",
    "        trgs.append(torch.tensor(row[\"trg\"]).to(device))\n",
    "\n",
    "    padded_srcs = pad_sequence(srcs, padding_value=PAD_IDX)\n",
    "    padded_trgs = pad_sequence(trgs, padding_value=PAD_IDX)\n",
    "    return {\"src\": padded_srcs, \"trg\": padded_trgs}\n",
    "\n",
    "train_dataloader = DataLoader(nmtds_train, batch_size=hyp_params['batch_size'], shuffle=True,\n",
    "                              collate_fn=lambda batch_size: collate_fn(batch_size, device))\n",
    "\n",
    "valid_dataloader = DataLoader(nmtds_valid, batch_size=hyp_params['batch_size'], shuffle=True,\n",
    "                              collate_fn=lambda batch_size: collate_fn(batch_size, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe2e61ed-c875-4a9a-9c57-60a9b82ee18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params[\"input_size_encoder\"] = len(nmtds_train.src_vocab)\n",
    "hyp_params[\"input_size_decoder\"] = len(nmtds_train.trg_vocab)\n",
    "hyp_params[\"output_size\"] = len(nmtds_train.trg_vocab)\n",
    "\n",
    "model = SeqtoSeq(hyp_params, target_vocab=nmtds_train.trg_vocab, device=device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "pad_idx = nmtds_train.trg_vocab[\"<pad>\"]\n",
    "criterion = nn.NLLLoss(ignore_index=pad_idx).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e89c4c12-8ad2-45cc-8d38-38a8e0af9e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [40:01<00:00,  5.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2401.5422139167786"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "epoch_loss = 0\n",
    "for epoch in range(hyp_params[\"num_epochs\"]):\n",
    "    for batch_idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        src = batch[\"src\"]  # shape --> e.g. (19, 2) sentence len, batch size\n",
    "        trg = batch[\"trg\"]  # shape --> e.g. (3, 2) sentence len, batch size\n",
    "\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass the input and target for model's forward method\n",
    "        # Shape --> (sentence len of TRG, batch size, vocab size) e.g (3, 2, 196)\n",
    "        # Explanation:\n",
    "        #    It just outputs probabilities for every single word in our vocab\n",
    "        #    for each word in sentence and each sentence in batch size\n",
    "        output = model(src, trg)\n",
    "\n",
    "        # Updating output shape --> [sentence length * batch size , vocab size]\n",
    "        # e.g (6, 196)\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "\n",
    "        # sentence len  * batch size\n",
    "        target = trg.reshape(-1)\n",
    "\n",
    "        # Calculate the loss value for every epoch\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Calculate the gradients for weights & biases using back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Clip the gradient value is it exceeds > 1\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the weights values\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ec6b1aaa-45a9-4a55-9356-92cd8f32d5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anyio==3.3.0\n",
      "appnope==0.1.2\n",
      "argon2-cffi==20.1.0\n",
      "async-generator==1.10\n",
      "attrs==21.2.0\n",
      "Babel==2.9.1\n",
      "backcall==0.2.0\n",
      "bleach==4.1.0\n",
      "blis==0.7.4\n",
      "catalogue==2.0.6\n",
      "certifi==2021.5.30\n",
      "cffi==1.14.6\n",
      "charset-normalizer==2.0.4\n",
      "click==7.1.2\n",
      "contextvars==2.4\n",
      "cymem==2.0.5\n",
      "dataclasses==0.8\n",
      "de-core-news-sm @ https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.1.0/de_core_news_sm-3.1.0-py3-none-any.whl\n",
      "decorator==5.0.9\n",
      "defusedxml==0.7.1\n",
      "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl\n",
      "entrypoints==0.3\n",
      "idna==3.2\n",
      "immutables==0.16\n",
      "importlib-metadata==4.6.4\n",
      "ipykernel==5.5.5\n",
      "ipython==7.16.1\n",
      "ipython-genutils==0.2.0\n",
      "jedi==0.18.0\n",
      "Jinja2==3.0.1\n",
      "json5==0.9.6\n",
      "jsonschema==3.2.0\n",
      "jupyter-client==7.0.1\n",
      "jupyter-core==4.7.1\n",
      "jupyter-server==1.10.2\n",
      "jupyterlab==3.1.9\n",
      "jupyterlab-pygments==0.1.2\n",
      "jupyterlab-server==2.7.2\n",
      "MarkupSafe==2.0.1\n",
      "mistune==0.8.4\n",
      "murmurhash==1.0.5\n",
      "nbclassic==0.3.1\n",
      "nbclient==0.5.4\n",
      "nbconvert==6.0.7\n",
      "nbformat==5.1.3\n",
      "nest-asyncio==1.5.1\n",
      "notebook==6.4.3\n",
      "numpy==1.19.5\n",
      "packaging==21.0\n",
      "pandocfilters==1.4.3\n",
      "parso==0.8.2\n",
      "pathy==0.6.0\n",
      "pexpect==4.8.0\n",
      "pickleshare==0.7.5\n",
      "preshed==3.0.5\n",
      "prometheus-client==0.11.0\n",
      "prompt-toolkit==3.0.20\n",
      "ptyprocess==0.7.0\n",
      "pycparser==2.20\n",
      "pydantic==1.8.2\n",
      "Pygments==2.10.0\n",
      "pyparsing==2.4.7\n",
      "pyrsistent==0.18.0\n",
      "python-dateutil==2.8.2\n",
      "pytz==2021.1\n",
      "pyzmq==22.2.1\n",
      "requests==2.26.0\n",
      "requests-unixsocket==0.2.0\n",
      "Send2Trash==1.8.0\n",
      "six==1.16.0\n",
      "smart-open==5.2.0\n",
      "sniffio==1.2.0\n",
      "spacy==3.1.2\n",
      "spacy-legacy==3.0.8\n",
      "srsly==2.4.1\n",
      "terminado==0.11.1\n",
      "testpath==0.5.0\n",
      "thinc==8.0.8\n",
      "torch==1.9.0\n",
      "torchtext==0.10.0\n",
      "tornado==6.1\n",
      "tqdm==4.62.2\n",
      "traitlets==4.3.3\n",
      "typer==0.3.2\n",
      "typing-extensions==3.10.0.0\n",
      "urllib3==1.26.6\n",
      "wasabi==0.8.2\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.2.1\n",
      "zipp==3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c5275152-77a2-48fe-a87d-ec6d0bf7885c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac3cbeb-eee4-4c24-9046-3b81a03189b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
