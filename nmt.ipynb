{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28602591-bd9f-467c-a015-11acf39a5665",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71e62732-00db-4b4e-8a3e-69be08de0fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58b39d2b-bbf2-4235-ad90-af8ee6d59ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea59b269-3071-4785-9651-fe811872aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params = {\n",
    "    \"batch_size\": 128,\n",
    "    \"num_epochs\": 100,\n",
    "\n",
    "    # Encoder parameters\n",
    "    \"encoder_embedding_size\": 256,\n",
    "    \"encoder_dropout\": 0.5,\n",
    "\n",
    "    # Decoder parameters\n",
    "    \"decoder_dropout\": 0.5,\n",
    "    \"decoder_embedding_size\": 256,\n",
    "\n",
    "    # Common parameters\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layers\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77f80e9c-9731-4b1c-b4f3-639e93958911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        \n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "\n",
    "        return hidden_state, cell_state\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        # as we are not feeding whole sentence we will each token a time\n",
    "        # hence our sequence length would be just 1 however shape of x is batch_size\n",
    "        # to add sequence length we will unsequeeze it\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedded, (hidden_state, cell_state))\n",
    "\n",
    "        # Shape --> predictions  (1, batch_size, target_vocab_size)\n",
    "        predictions = self.fc(outputs.squeeze(0))\n",
    "\n",
    "        return predictions, hidden_state, cell_state\n",
    "\n",
    "class SeqtoSeq(nn.Module):\n",
    "    def __init__(self, gen_params, target_vocab, device):\n",
    "        super(SeqtoSeq, self).__init__()\n",
    "\n",
    "        self.Encoder = Encoder(gen_params[\"input_size_encoder\"],\n",
    "                          gen_params[\"encoder_embedding_size\"],\n",
    "                          gen_params[\"hidden_size\"],\n",
    "                          gen_params[\"num_layers\"],\n",
    "                          gen_params[\"encoder_dropout\"]).to(device)\n",
    "\n",
    "        self.Decoder = Decoder(gen_params[\"input_size_decoder\"],\n",
    "                          gen_params[\"decoder_embedding_size\"],\n",
    "                          gen_params[\"hidden_size\"],\n",
    "                          gen_params[\"num_layers\"],\n",
    "                          gen_params[\"decoder_dropout\"],\n",
    "                          gen_params[\"output_size\"]).to(device)\n",
    "\n",
    "        self.target_vocab = target_vocab\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, source, target, tfr=0.5):\n",
    "        # Shape -> (Sentence length, Batch_size)\n",
    "        batch_size = source.shape[1]\n",
    "\n",
    "        target_len = target.shape[0]  # Length of target sentences\n",
    "        target_vocab_size = len(self.target_vocab)\n",
    "        \n",
    "        # here we will store all the outputs\n",
    "        # so outputs is arrange in a way that sentences are in column and batch size is row and every element\n",
    "        # will consist of probability of each word from the vocab\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
    "\n",
    "        # Shape --> (hs, cs) (num_layers, batch_size size, hidden_size) (contains encoder's hs, cs - context vectors)\n",
    "        hidden_state, cell_state = self.Encoder(source)\n",
    "\n",
    "        # Shape of x (32 elements)\n",
    "        x = target[0]  # First token (Trigger)\n",
    "\n",
    "        for i in range(1, target_len):\n",
    "            # Shape --> output (batch_size, target_vocab_size)\n",
    "            output, hidden_state, cell_state = self.Decoder(x, hidden_state, cell_state)\n",
    "            outputs[i] = output\n",
    "            best_guess = output.argmax(1)  # 0th dimension is batch size, 1st dimension is word embedding\n",
    "            # Schedule sampling\n",
    "            x = target[\n",
    "                i] if random.random() < tfr else best_guess  # Either pass the next word correctly from the dataset\n",
    "            # or use the earlier predicted word\n",
    "\n",
    "        # Shape --> (sentence length, batch size, vocab size)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "772e5645-da1e-4ac6-9bd1-ab3e2ffb13f0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "class nmtDataset(Dataset):\n",
    "    def __init__(self, ds_path, split, train_ds=None):\n",
    "        self.en = open(ds_path + split + '.en', encoding='utf-8').readlines()\n",
    "        self.de = open(ds_path + split + '.de', encoding='utf-8').readlines()\n",
    "        \n",
    "        self.tokenizers = { 'en': get_tokenizer('spacy', language='en_core_web_sm'), \n",
    "                            'de': get_tokenizer('spacy', language='de_core_news_sm') }\n",
    "        \n",
    "        if split == 'train':\n",
    "            self.src_vocab, self.trg_vocab = self._build_vocab()\n",
    "        else:\n",
    "            self.src_vocab, self.trg_vocab = train_ds.src_vocab, train_ds.trg_vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.en)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        src_tokens = self.tokenizers['en'](self.en[item].lower().strip())\n",
    "        trg_tokens = self.tokenizers['de'](self.de[item].lower().strip())\n",
    "  \n",
    "        return {\n",
    "            \"src\": [self.src_vocab['<sos>']] + self.src_vocab.lookup_indices(src_tokens) + [self.src_vocab['<eos>']],\n",
    "            \"trg\": [self.trg_vocab['<sos>']] + self.trg_vocab.lookup_indices(trg_tokens) + [self.trg_vocab['<eos>']]\n",
    "        }\n",
    "        \n",
    "    def _build_vocab(self):\n",
    "        src_vocab = build_vocab_from_iterator(self._get_tokens(self.en, 'en'), specials=['<unk>', '<pad>', '<sos>', '<eos>'], min_freq=2)  # discarding words occurs < 2 times\n",
    "        trg_vocab = build_vocab_from_iterator(self._get_tokens(self.de, 'de'), specials=['<unk>', '<pad>', '<sos>', '<eos>'], min_freq=2)\n",
    "  \n",
    "        trg_vocab.set_default_index(trg_vocab['<unk>'])\n",
    "        src_vocab.set_default_index(src_vocab['<unk>'])\n",
    "\n",
    "        return src_vocab, trg_vocab\n",
    "    \n",
    "    def _get_tokens(self, corpus, lang):\n",
    "        for line in corpus:\n",
    "            yield self.tokenizers[lang](line.lower().strip())\n",
    "    \n",
    "nmtds_train = nmtDataset('datasets/Multi30k/', 'train')\n",
    "nmtds_valid = nmtDataset('datasets/Multi30k/', 'val', nmtds_train)\n",
    "nmtds_test = nmtDataset('datasets/Multi30k/', 'test', nmtds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73779a22-9baa-49d5-a298-13b1abe6d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_fn(batch, device):\n",
    "    PAD_IDX = 1\n",
    "    trgs = []\n",
    "    srcs = []\n",
    "    for row in batch:\n",
    "        srcs.append(torch.tensor(row[\"src\"]).to(device))\n",
    "        trgs.append(torch.tensor(row[\"trg\"]).to(device))\n",
    "\n",
    "    padded_srcs = pad_sequence(srcs, padding_value=PAD_IDX)\n",
    "    padded_trgs = pad_sequence(trgs, padding_value=PAD_IDX)\n",
    "    return {\"src\": padded_srcs, \"trg\": padded_trgs}\n",
    "\n",
    "train_dataloader = DataLoader(nmtds_train, batch_size=hyp_params['batch_size'], shuffle=True,\n",
    "                              collate_fn=lambda batch_size: collate_fn(batch_size, device))\n",
    "\n",
    "valid_dataloader = DataLoader(nmtds_valid, batch_size=hyp_params['batch_size'], shuffle=True,\n",
    "                              collate_fn=lambda batch_size: collate_fn(batch_size, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1968b818-4167-4b8f-be1b-9beb109fb29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generator(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(data_loader):\n",
    "            src = batch[\"src\"]  # shape --> e.g. (19, 2) sentence len, batch size\n",
    "            trg = batch[\"trg\"]  # shape --> e.g. (3, 2) sentence len, batch size\n",
    "\n",
    "            # Pass the input and target for model's forward method\n",
    "            # Shape --> (sentence len of TRG, batch size, vocab size) e.g (3, 2, 196)\n",
    "            # Explanation:\n",
    "            #    It just outputs probabilities for every single word in our vocab\n",
    "            #    for each word in sentence and each sentence in batch size\n",
    "            output = model(src, trg, 0)\n",
    "\n",
    "            # Updating output shape --> [sentence length * batch size , vocab size]\n",
    "            # e.g (6, 196)\n",
    "            output = output.reshape(-1, output.shape[2])\n",
    "\n",
    "            # sentence len  * batch size\n",
    "            target = trg.reshape(-1)\n",
    "\n",
    "            # Calculate the loss value for every epoch\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss/len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe2e61ed-c875-4a9a-9c57-60a9b82ee18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params[\"input_size_encoder\"] = len(nmtds_train.src_vocab)\n",
    "hyp_params[\"input_size_decoder\"] = len(nmtds_train.trg_vocab)\n",
    "hyp_params[\"output_size\"] = len(nmtds_train.trg_vocab)\n",
    "\n",
    "model = SeqtoSeq(hyp_params, target_vocab=nmtds_train.trg_vocab, device=device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "pad_idx = nmtds_train.trg_vocab[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c4c12-8ad2-45cc-8d38-38a8e0af9e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_el = math.inf\n",
    "patience = 1\n",
    "best_model = {}\n",
    "best_epoch = 0\n",
    "\n",
    "epoch_loss = 0\n",
    "for epoch in range(hyp_params[\"num_epochs\"]):\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        src = batch[\"src\"]  # shape --> e.g. (19, 2) sentence len, batch size\n",
    "        trg = batch[\"trg\"]  # shape --> e.g. (3, 2) sentence len, batch size\n",
    "\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass the input and target for model's forward method\n",
    "        # Shape --> (sentence len of TRG, batch size, vocab size) e.g (3, 2, 196)\n",
    "        # Explanation:\n",
    "        #    It just outputs probabilities for every single word in our vocab\n",
    "        #    for each word in sentence and each sentence in batch size\n",
    "        output = model(src, trg)\n",
    "\n",
    "        # Updating output shape --> [sentence length * batch size , vocab size]\n",
    "        # e.g (6, 196)\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "\n",
    "        # sentence len  * batch size\n",
    "        target = trg.reshape(-1)\n",
    "\n",
    "        # Calculate the loss value for every epoch\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Calculate the gradients for weights & biases using back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient value is it exceeds > 1\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Update the weights values\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_loss /= len(train_dataloader)\n",
    "    eval_loss = evaluate_generator(model, valid_dataloader, criterion)\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}, Train loss: {epoch_loss}, Eval loss: {eval_loss}, patience: {patience}\")\n",
    "    \n",
    "    if eval_loss < min_el:\n",
    "        best_epoch = epoch\n",
    "        min_el = eval_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'eval_loss': min_el\n",
    "        }, 'model-vanilla.pt')\n",
    "        patience = 1\n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    if patience == 10:\n",
    "        print(\"[STOPPING] Early stopping in action..\")\n",
    "        print(f\"Best epoch was {best_epoch} with {min_el} eval loss\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "009685eb-3142-4bdf-b05e-5d89faf0f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(snt, dataset, model):\n",
    "    tokens = dataset.tokenizers['en'](snt.lower().strip())\n",
    "    indices = [dataset.src_vocab['<sos>']] + dataset.src_vocab.lookup_indices(tokens) + [dataset.src_vocab['<eos>']]\n",
    "    inp_tensor = torch.tensor(indices).unsqueeze(1).to(device)\n",
    "\n",
    "    # Build encoder hidden, cell state\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.Encoder(inp_tensor)\n",
    "\n",
    "    outputs = [dataset.trg_vocab[\"<sos>\"]]\n",
    "\n",
    "    for _ in range(50):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.Decoder(previous_word, hidden, cell)\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Model predicts it's the end of the sentence\n",
    "        if output.argmax(1).item() == dataset.trg_vocab['<eos>']:\n",
    "            break\n",
    "\n",
    "    return dataset.trg_vocab.lookup_tokens(outputs)\n",
    "\n",
    "def bleu(model, dataset, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "\n",
    "    for example in tqdm(dataset):\n",
    "        src = example[\"src\"][1:-1]\n",
    "        trg = example[\"trg\"][1:-1]\n",
    "        \n",
    "        src = ' '.join(dataset.src_vocab.lookup_tokens(src))\n",
    "        trg = dataset.trg_vocab.lookup_tokens(trg)\n",
    "\n",
    "        prediction = translate(src, dataset, model)\n",
    "        prediction = prediction[1:-1]  # remove <eos> token\n",
    "        \n",
    "        targets.append([trg])\n",
    "        outputs.append(prediction)\n",
    "\n",
    "    return bleu_score(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86a2bd3d-1eac-41c5-a4f3-57deb26f8237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_l = SeqtoSeq(hyp_params, target_vocab=nmtds_train.trg_vocab, device=device)\n",
    "model_l.load_state_dict(torch.load('model-vanilla.pt', map_location=device)[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffb2c650-6614-4034-9b4a-12b828bb6a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:43<00:00, 23.06it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16437269271063837"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu(model_l, nmtds_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7be5b477-3d87-42ce-8030-9eee13be8e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.964359372854233"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('model-vanilla.pt', map_location=device)[\"eval_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0966684d-240e-4a4b-886b-c1410e901299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
