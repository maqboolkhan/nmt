{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef6af3f3-bb66-4fdc-bb16-e6a509cdb8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from math import log\n",
    "from copy import deepcopy\n",
    "\n",
    "from dataset import nmtDataset\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1f185ca-f131-48f4-a6ec-7d51cf027262",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "333dc1b6-68ac-4c62-a692-a19f7a1ab390",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params = {\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 10,\n",
    "\n",
    "    # Encoder parameters\n",
    "    \"encoder_embedding_size\": 512,\n",
    "    \"encoder_dropout\": 0, # Disabled dropout because now we are only using single layer LSTM\n",
    "\n",
    "    # Decoder parameters\n",
    "    \"decoder_dropout\": 0,\n",
    "    \"decoder_embedding_size\": 512,\n",
    "\n",
    "    # Common parameters\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layers\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7300ca40-c586-49ee-aa37-2df1eb3e6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # Shape (hidden): --> [num_layers * 2, batch_size, hidden_size]\n",
    "        # encoder_outputs: Shape --> [Sequence_length , batch_size , hidden_size * 2]\n",
    "        \n",
    "        # Making hidden layer -> [batch size, hidden_size]\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        # Repeating hidden state for every sentences to the maximum sentence length\n",
    "        # Shape (hidden): --> [batch size, src len, hidden_size]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # Sentences are in column (see enc_out shape), so instead we want each sentence\n",
    "        # in a row. Hence, we are permuting it\n",
    "        # Shape (encoder_outputs): --> [batch size, src len, hidden_size * 2]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        # Concatenation will put hidden states with relavant sentence and with the each\n",
    "        # word of the sentence\n",
    "        # Shape (energy): --> [batch size, src len, hidden_size]\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        # Shape (attention): --> [batch size, src len]\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61b3e289-a32f-43d7-8000-85d1ff40df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=dropout, bidirectional = True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Shape (embedding) --> [Sequence_length , batch_size , embedding dims]\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # ************** Multiplied by 2 because of bi-directional LSTM\n",
    "        # Shape --> (output) [Sequence_length , batch_size , hidden_size * 2]\n",
    "        # Shape --> (hs, cs) [num_layers * 2, batch_size, hidden_size]\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "        \n",
    "        return outputs, hidden_state, cell_state\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = Attention(hidden_size, hidden_size)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "         # ************** Multiplying 2 because of bi-directional LSTM\n",
    "        self.LSTM = nn.LSTM((hidden_size * 2) + embedding_dim, hidden_size, num_layers * 2, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear((hidden_size* 2) + hidden_size + embedding_dim, output_size)\n",
    "    \n",
    "    def forward(self, x, enc_outputs, hidden_state, cell_state, mask):\n",
    "        # As we are not feeding whole sentence we will each token a time\n",
    "        # hence our sequence length would be just 1 however shape of x is batch_size\n",
    "        # to add sequence length we will unsequeeze it\n",
    "        # Shape (x) --> [batch_size] (see seq2seq model) so making it [1, batch_size]\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        # Shape (embedded) --> (1, batch_size, embedding dims)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # Shape (a): --> [batch_size, src len]\n",
    "        a = self.attention(hidden_state, enc_outputs, mask)\n",
    "        \n",
    "        # Shape (a): --> [batch_size, 1, src len]\n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        # Shape (encoder_outputs): --> [batch_size, src len, hidden_size * 2]\n",
    "        enc_outputs = enc_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        # Shape (weighted): --> [batch_size, 1, hidden_size * 2]\n",
    "        weighted = torch.bmm(a, enc_outputs)\n",
    "        \n",
    "        # Shape (weighted): --> [1, batch_size, hidden_size * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        # Shape (rnn_input): --> [1, batch_size, (hidden_size * 2) + embedding dims]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        \n",
    "        # Shape (output): --> [1, batch_size, hidden_size]\n",
    "        # Shape (hidden_state, cell_state): --> [num_layers * num_directions, batch_size, hidden_size]\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(rnn_input, (hidden_state, cell_state))\n",
    "        \n",
    "        embedded = embedded.squeeze(0) # Shape (output): --> [batch_size, hidden_size]\n",
    "        outputs = outputs.squeeze(0) # Shape (output): --> [batch_size, hidden_size]\n",
    "        weighted = weighted.squeeze(0) # Shape (output): --> [batch_size, hidden_size * 2]\n",
    "        \n",
    "        # Shape (predictions): --> [batch_size, output_size]\n",
    "        predictions = self.fc(torch.cat((outputs, weighted, embedded), dim = 1))\n",
    "        \n",
    "        return predictions, hidden_state, cell_state, a.squeeze(1)\n",
    "\n",
    "class SeqtoSeq(nn.Module):\n",
    "    def __init__(self, gen_params, target_vocab, src_pad_idx, device):\n",
    "        super(SeqtoSeq, self).__init__()\n",
    "\n",
    "        self.Encoder = Encoder(gen_params[\"input_size_encoder\"],\n",
    "                          gen_params[\"encoder_embedding_size\"],\n",
    "                          gen_params[\"hidden_size\"],\n",
    "                          gen_params[\"num_layers\"],\n",
    "                          gen_params[\"encoder_dropout\"]).to(device)\n",
    "\n",
    "        self.Decoder = Decoder(gen_params[\"input_size_decoder\"],\n",
    "                          gen_params[\"decoder_embedding_size\"],\n",
    "                          gen_params[\"hidden_size\"],\n",
    "                          gen_params[\"num_layers\"],\n",
    "                          gen_params[\"decoder_dropout\"],\n",
    "                          gen_params[\"output_size\"]).to(device)\n",
    "\n",
    "        self.target_vocab = target_vocab\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "    \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, source, target, tfr=0.5):\n",
    "        # (source) Shape -> (Sentence length, Batch_size)\n",
    "        batch_size = source.shape[1]\n",
    "\n",
    "        target_len = target.shape[0]  # Length of target sentences\n",
    "        target_vocab_size = len(self.target_vocab)\n",
    "        \n",
    "        # here we will store all the outputs\n",
    "        # so outputs is arrange in a way that sentences are in column and batch size is row and every element\n",
    "        # will consist of probability of each word from the vocab\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
    "\n",
    "        # Shape --> (hs, cs) (num_layers * 2, batch_size size, hidden_size) (contains encoder's hs, cs - context vectors)\n",
    "        enc_outputs, hidden_state, cell_state = self.Encoder(source)\n",
    "\n",
    "        # Shape (target) -> (Sentence length, Batch_size)\n",
    "        # Shape (x) --> (batch_size)\n",
    "        x = target[0]  # First token (Trigger)\n",
    "        \n",
    "        mask = self.create_mask(source)\n",
    "        \n",
    "        for i in range(1, target_len):\n",
    "            # Shape (output) --> (batch_size, target_vocab_size)\n",
    "            # Shape (hs, cl) --> (num_layers * 2, batch_size , hidden_size)\n",
    "            # _ is attention which we dont need here!\n",
    "            output, hidden_state, cell_state, _ = self.Decoder(x, enc_outputs, hidden_state, cell_state, mask)\n",
    "            outputs[i] = output\n",
    "            best_guess = output.argmax(1)  # 0th dimension is batch size, 1st dimension is word embedding\n",
    "            # Schedule sampling\n",
    "            x = target[\n",
    "                i] if random.random() < tfr else best_guess  # Either pass the next word correctly from the dataset\n",
    "            # or use the earlier predicted word\n",
    "\n",
    "        # Shape --> (sentence length, batch size, vocab size)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21984b8f-284c-481c-9604-e982796dd299",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmtds_train = nmtDataset('datasets/Multi30k/', 'train')\n",
    "nmtds_test = nmtDataset('datasets/Multi30k/', 'test', nmtds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b66ca14f-1b36-4842-90d4-ea97e492fa53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_params[\"input_size_encoder\"] = len(nmtds_train.src_vocab)\n",
    "hyp_params[\"input_size_decoder\"] = len(nmtds_train.trg_vocab)\n",
    "hyp_params[\"output_size\"] = len(nmtds_train.trg_vocab)\n",
    "pad_idx = nmtds_train.trg_vocab[\"<pad>\"]\n",
    "\n",
    "model_l = SeqtoSeq(hyp_params, nmtds_train.trg_vocab, pad_idx, device=device)\n",
    "model_l.load_state_dict(torch.load('model-attention.pt', map_location=device)[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c31081c1-f8bc-4314-a440-4a5661c502bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(snt, dataset, model, attention, device):\n",
    "    tokens = dataset.tokenizers['en'](snt.lower().strip())\n",
    "    indices = [dataset.src_vocab['<sos>']] + dataset.src_vocab.lookup_indices(tokens) + [dataset.src_vocab['<eos>']]\n",
    "    inp_tensor = torch.tensor(indices).unsqueeze(1).to(device)\n",
    "\n",
    "    # Build encoder hidden, cell state\n",
    "    with torch.no_grad():\n",
    "        eouts, hidden, cell = model.Encoder(inp_tensor)\n",
    "\n",
    "    start = dataset.trg_vocab[\"<sos>\"]\n",
    "    beam_width = 2\n",
    "    beam = []\n",
    "    \n",
    "    for _ in range(50):\n",
    "        with torch.no_grad():\n",
    "            mask = model.create_mask(inp_tensor)\n",
    "\n",
    "            if len(beam) == 0:\n",
    "                previous_word = torch.LongTensor([start]).to(device)\n",
    "                output, hidden, cell, _ = model.Decoder(previous_word, eouts, hidden, cell, mask)\n",
    "                \n",
    "                probs = output.topk(2).values.squeeze().numpy()\n",
    "                vals = output.topk(2).indices.squeeze().numpy()\n",
    "\n",
    "                for p, v in zip(probs, vals):\n",
    "                    beam.append([{\"indices\": v, \"prob\": p, \"h\": hidden, \"c\": cell}, log(p)])\n",
    "            elif len(beam) > 0:\n",
    "                temp_beam = []\n",
    "                both_eos = 0\n",
    "                for w in beam:\n",
    "                    previous_word = torch.LongTensor([w[-2][\"indices\"]]).to(device)\n",
    "                    output, hidden, cell, _ = model.Decoder(previous_word, eouts, w[-2][\"h\"], w[-2][\"c\"], mask)\n",
    "                    \n",
    "                    print(output.shape)\n",
    "\n",
    "                    probs = output.topk(2).values.squeeze().numpy()\n",
    "                    vals = output.topk(2).indices.squeeze().numpy()\n",
    "                    for p, v in zip(probs, vals):\n",
    "                        temp_beam.append(w[:-1] + [{\"indices\": v, \"prob\": p, \"h\": hidden, \"c\": cell}, log(p) + w[-1]])\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                temp_beam.sort(key=lambda x: x[-1])\n",
    "                beam = temp_beam[-2:]\n",
    "                \n",
    "                \n",
    "\n",
    "    \n",
    "    beam[0] = [{ \"p\": d[\"prob\"], \"i\": d[\"indices\"] } for d in beam[0][:-1] if d[\"indices\"] != dataset.trg_vocab['<eos>']]\n",
    "    beam[1] = [{ \"p\": d[\"prob\"], \"i\": d[\"indices\"] } for d in beam[1][:-1] if d[\"indices\"] != dataset.trg_vocab['<eos>']]\n",
    "    \n",
    "    if sum([p[\"p\"] for p in beam[0]]) > sum([p[\"p\"] for p in beam[1]]):\n",
    "        res = [i[\"i\"] for i in beam[0]]\n",
    "    else:\n",
    "        res = [i[\"i\"] for i in beam[1]]\n",
    "    \n",
    "    return dataset.trg_vocab.lookup_tokens(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd1c6115-0160-43fa-b6aa-4e240acf6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(model, dataset, attention, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "\n",
    "    for example in tqdm(dataset):\n",
    "        src = example[\"src\"][1:-1]\n",
    "        trg = example[\"trg\"][1:-1]\n",
    "        \n",
    "        src = ' '.join(dataset.src_vocab.lookup_tokens(src))\n",
    "        trg = dataset.trg_vocab.lookup_tokens(trg)\n",
    "\n",
    "        prediction = translate(src, dataset, model, attention, device)\n",
    "        prediction = prediction[1:-1]  # remove <eos> token\n",
    "        \n",
    "        targets.append([trg])\n",
    "        outputs.append(prediction)\n",
    "\n",
    "    return bleu_score(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2df316d8-9b84-45f0-bd00-8e5f851a8c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                              | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                                                                                                                                                                                                                                                                                     | 1/1000 [00:00<11:36,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▋                                                                                                                                                                                                                                                                                                                                     | 2/1000 [00:01<10:58,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▉                                                                                                                                                                                                                                                                                                                                     | 3/1000 [00:01<10:46,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▉                                                                                                                                                                                                                                                                                                                                     | 3/1000 [00:02<11:38,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n",
      "torch.Size([1, 7853])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gb/6w65rmfj6ll963jzc51njw880000gn/T/ipykernel_45343/2428166803.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnmtds_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/gb/6w65rmfj6ll963jzc51njw880000gn/T/ipykernel_45343/1365990534.py\u001b[0m in \u001b[0;36mbleu\u001b[0;34m(model, dataset, attention, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# remove <eos> token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/gb/6w65rmfj6ll963jzc51njw880000gn/T/ipykernel_45343/3687326340.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(snt, dataset, model, attention, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                     \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                         \u001b[0mtemp_beam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"indices\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prob\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"h\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bleu(model_l, nmtds_test, True, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8e699f5-5eb5-42d8-b12b-907c064452c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "a = torch.tensor([  [0.1, 0.7, 0.1, 0.1],\n",
    "                    [0.7, 0.1, 0.1, 0.1],\n",
    "                    [0.1, 0.1, 0.6, 0.2],\n",
    "                    [0.1, 0.1, 0.1, 0.7],\n",
    "                    [0.4, 0.3, 0.2, 0.1]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aad76db-fbdf-4bab-914a-14fa31d090c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f8b13ab-820c-4679-a958-2c8a2ffd5740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d7625c-34e1-4f6b-b918-27a8829f36d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe7da2-dd80-4bf9-a04e-628434f55c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
