{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e62732-00db-4b4e-8a3e-69be08de0fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from dataset import nmtDataset\n",
    "import helpers as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3219329c-5bd5-4b2b-8c04-a2b6442b398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b39d2b-bbf2-4235-ad90-af8ee6d59ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea59b269-3071-4785-9651-fe811872aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params = {\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 10,\n",
    "\n",
    "    # Encoder parameters\n",
    "    \"encoder_embedding_size\": 512,\n",
    "    \"encoder_dropout\": 0, # Disabled dropout because now we are only using single layer LSTM\n",
    "\n",
    "    # Decoder parameters\n",
    "    \"decoder_dropout\": 0,\n",
    "    \"decoder_embedding_size\": 512,\n",
    "\n",
    "    # Common parameters\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layers\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7daf9a0c-37b3-4dad-9d85-0c2ad4bdd89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = utils.Logger('logs/emd512-enc2-dec2--attention.out')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e5e16cd-2f89-4650-922d-bced19c6659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # Shape (hidden): --> [num_layers * 2, batch_size, hidden_size]\n",
    "        # encoder_outputs: Shape --> [Sequence_length , batch_size , hidden_size * 2]\n",
    "        \n",
    "        # Making hidden layer -> [batch size, hidden_size]\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        # Repeating hidden state for every sentences to the maximum sentence length\n",
    "        # Shape (hidden): --> [batch size, src len, hidden_size]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # Sentences are in column (see enc_out shape), so instead we want each sentence\n",
    "        # in a row. Hence, we are permuting it\n",
    "        # Shape (encoder_outputs): --> [batch size, src len, hidden_size * 2]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        # Concatenation will put hidden states with relavant sentence and with the each\n",
    "        # word of the sentence\n",
    "        # Shape (energy): --> [batch size, src len, hidden_size]\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        # Shape (attention): --> [batch size, src len]\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77f80e9c-9731-4b1c-b4f3-639e93958911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=dropout, bidirectional = True)\n",
    "        \n",
    "    def forward(self, x, src_lens):\n",
    "        # Shape (embedding) --> [Sequence_length , batch_size , embedding dims]\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        \n",
    "        packed_embedded = pack_padded_sequence(embedding, src_lens.to('cpu'), enforce_sorted=False)\n",
    "        \n",
    "        # ************** Multiplied by 2 because of bi-directional LSTM\n",
    "        # Shape --> (output) [Sequence_length , batch_size , hidden_size * 2]\n",
    "        # Shape --> (hs, cs) [num_layers * 2, batch_size, hidden_size]\n",
    "        packed_outputs, (hidden_state, cell_state) = self.LSTM(packed_embedded)\n",
    "        \n",
    "        outputs, _ = pad_packed_sequence(packed_outputs)\n",
    "        \n",
    "        return outputs, hidden_state, cell_state\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = Attention(hidden_size, hidden_size)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "         # ************** Multiplying 2 because of bi-directional LSTM\n",
    "        self.LSTM = nn.LSTM((hidden_size * 2) + embedding_dim, hidden_size, num_layers * 2, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear((hidden_size* 2) + hidden_size + embedding_dim, output_size)\n",
    "    \n",
    "    def forward(self, x, enc_outputs, hidden_state, cell_state, mask):\n",
    "        # As we are not feeding whole sentence we will each token a time\n",
    "        # hence our sequence length would be just 1 however shape of x is batch_size\n",
    "        # to add sequence length we will unsequeeze it\n",
    "        # Shape (x) --> [batch_size] (see seq2seq model) so making it [1, batch_size]\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        # Shape (embedded) --> (1, batch_size, embedding dims)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # Shape (a): --> [batch_size, src len]\n",
    "        a = self.attention(hidden_state, enc_outputs, mask)\n",
    "        \n",
    "        # Shape (a): --> [batch_size, 1, src len]\n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        # Shape (encoder_outputs): --> [batch_size, src len, hidden_size * 2]\n",
    "        enc_outputs = enc_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        # Shape (weighted): --> [batch_size, 1, hidden_size * 2]\n",
    "        weighted = torch.bmm(a, enc_outputs)\n",
    "        \n",
    "        # Shape (weighted): --> [1, batch_size, hidden_size * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        # Shape (rnn_input): --> [1, batch_size, (hidden_size * 2) + embedding dims]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        # Shape (output): --> [1, batch_size, hidden_size]\n",
    "        # Shape (hidden_state, cell_state): --> [num_layers * num_directions, batch_size, hidden_size]\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(rnn_input, (hidden_state, cell_state))\n",
    "        \n",
    "        embedded = embedded.squeeze(0) # Shape (output): --> [batch_size, hidden_size]\n",
    "        outputs = outputs.squeeze(0) # Shape (output): --> [batch_size, hidden_size]\n",
    "        weighted = weighted.squeeze(0) # Shape (output): --> [batch_size, hidden_size * 2]\n",
    "        \n",
    "        # Shape (predictions): --> [batch_size, output_size]\n",
    "        predictions = self.fc(torch.cat((outputs, weighted, embedded), dim = 1))\n",
    "        \n",
    "        return predictions, hidden_state, cell_state, a.squeeze(1)\n",
    "\n",
    "class SeqtoSeq(nn.Module):\n",
    "    def __init__(self, gen_params, target_vocab, src_pad_idx, device):\n",
    "        super(SeqtoSeq, self).__init__()\n",
    "\n",
    "        self.Encoder = Encoder(gen_params[\"input_size_encoder\"],\n",
    "                          gen_params[\"encoder_embedding_size\"],\n",
    "                          gen_params[\"hidden_size\"],\n",
    "                          gen_params[\"num_layers\"],\n",
    "                          gen_params[\"encoder_dropout\"]).to(device)\n",
    "\n",
    "        self.Decoder = Decoder(gen_params[\"input_size_decoder\"],\n",
    "                          gen_params[\"decoder_embedding_size\"],\n",
    "                          gen_params[\"hidden_size\"],\n",
    "                          gen_params[\"num_layers\"],\n",
    "                          gen_params[\"decoder_dropout\"],\n",
    "                          gen_params[\"output_size\"]).to(device)\n",
    "\n",
    "        self.target_vocab = target_vocab\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "    \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, source, src_lens, target, tfr=0.5):\n",
    "        # (source) Shape -> (Sentence length, Batch_size)\n",
    "        batch_size = source.shape[1]\n",
    "\n",
    "        target_len = target.shape[0]  # Length of target sentences\n",
    "        target_vocab_size = len(self.target_vocab)\n",
    "        \n",
    "        # here we will store all the outputs\n",
    "        # so outputs is arrange in a way that sentences are in column and batch size is row and every element\n",
    "        # will consist of probability of each word from the vocab\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
    "\n",
    "        # Shape --> (hs, cs) (num_layers * 2, batch_size size, hidden_size) (contains encoder's hs, cs - context vectors)\n",
    "        enc_outputs, hidden_state, cell_state = self.Encoder(source, src_lens)\n",
    "\n",
    "        # Shape (target) -> (Sentence length, Batch_size)\n",
    "        # Shape (x) --> (batch_size)\n",
    "        x = target[0]  # First token (Trigger)\n",
    "        \n",
    "        mask = self.create_mask(source)\n",
    "        \n",
    "        for i in range(1, target_len):\n",
    "            # Shape (output) --> (batch_size, target_vocab_size)\n",
    "            # Shape (hs, cl) --> (num_layers * 2, batch_size , hidden_size)\n",
    "            # _ is attention which we dont need here!\n",
    "            output, hidden_state, cell_state, _ = self.Decoder(x, enc_outputs, hidden_state, cell_state, mask)\n",
    "            outputs[i] = output\n",
    "            best_guess = output.argmax(1)  # 0th dimension is batch size, 1st dimension is word embedding\n",
    "            # Schedule sampling\n",
    "            x = target[\n",
    "                i] if random.random() < tfr else best_guess  # Either pass the next word correctly from the dataset\n",
    "            # or use the earlier predicted word\n",
    "\n",
    "        # Shape --> (sentence length, batch size, vocab size)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "772e5645-da1e-4ac6-9bd1-ab3e2ffb13f0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "nmtds_train = nmtDataset('datasets/Multi30k/', 'train')\n",
    "nmtds_valid = nmtDataset('datasets/Multi30k/', 'val', nmtds_train)\n",
    "nmtds_test = nmtDataset('datasets/Multi30k/', 'test', nmtds_train)\n",
    "\n",
    "pad_idx = nmtds_train.trg_vocab[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73779a22-9baa-49d5-a298-13b1abe6d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(nmtds_train, batch_size=hyp_params['batch_size'], shuffle=True,\n",
    "                              collate_fn=lambda batch_size: utils.collate_fn(batch_size, pad_idx, device))\n",
    "\n",
    "valid_dataloader = DataLoader(nmtds_valid, batch_size=hyp_params['batch_size'], shuffle=True,\n",
    "                              collate_fn=lambda batch_size: utils.collate_fn(batch_size, pad_idx, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe2e61ed-c875-4a9a-9c57-60a9b82ee18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params[\"input_size_encoder\"] = len(nmtds_train.src_vocab)\n",
    "hyp_params[\"input_size_decoder\"] = len(nmtds_train.trg_vocab)\n",
    "hyp_params[\"output_size\"] = len(nmtds_train.trg_vocab)\n",
    "\n",
    "model = SeqtoSeq(hyp_params, nmtds_train.trg_vocab, pad_idx, device=device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e89c4c12-8ad2-45cc-8d38-38a8e0af9e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [04:30<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 3.97664475440979, Eval loss: 3.6309409141540527, patience: 1. Time 273.9009120464325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [04:27<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 2.700993776321411, Eval loss: 3.5215768814086914, patience: 1. Time 270.8571147918701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [04:27<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 2.161348819732666, Eval loss: 3.5366251468658447, patience: 1. Time 270.13098335266113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [04:25<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train loss: 1.8363069295883179, Eval loss: 3.642312526702881, patience: 2. Time 268.81717014312744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [04:25<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 1.6122450828552246, Eval loss: 3.7530057430267334, patience: 3. Time 268.07095766067505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [04:24<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train loss: 1.397431492805481, Eval loss: 3.9251084327697754, patience: 4. Time 267.5074694156647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [04:23<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train loss: 1.2243199348449707, Eval loss: 4.096966743469238, patience: 5. Time 266.7358281612396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [04:25<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train loss: 1.0842950344085693, Eval loss: 4.353078842163086, patience: 6. Time 268.07155418395996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [04:25<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train loss: 0.9771803021430969, Eval loss: 4.482593059539795, patience: 7. Time 268.75130581855774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 454/454 [04:27<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 0.9090154767036438, Eval loss: 4.63552188873291, patience: 8. Time 270.7253384590149\n"
     ]
    }
   ],
   "source": [
    "min_el = math.inf\n",
    "patience = 1\n",
    "best_model = {}\n",
    "best_epoch = 0\n",
    "\n",
    "epoch_loss = 0\n",
    "for epoch in range(hyp_params[\"num_epochs\"]):\n",
    "    start = time.time()\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    epoch_loss = utils.train_model(model, train_dataloader, criterion, optimizer)\n",
    "    eval_loss = utils.evaluate_model(model, valid_dataloader, criterion)\n",
    "    \n",
    "    log.log(f\"Epoch: {epoch+1}, Train loss: {epoch_loss}, Eval loss: {eval_loss}, patience: {patience}. Time {time.time() - start}\")\n",
    "\n",
    "    \n",
    "    if eval_loss < min_el:\n",
    "        best_epoch = epoch+1\n",
    "        min_el = eval_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'eval_loss': min_el\n",
    "        }, 'model-attention.pt')\n",
    "        patience = 1\n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    if patience == 10:\n",
    "        log.log(\"[STOPPING] Early stopping in action..\")\n",
    "        log.log(f\"Best epoch was {best_epoch} with {min_el} eval loss\")\n",
    "        break\n",
    "\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86a2bd3d-1eac-41c5-a4f3-57deb26f8237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqtoSeq(\n",
       "  (Encoder): Encoder(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (embedding): Embedding(5893, 512)\n",
       "    (LSTM): LSTM(512, 512, bidirectional=True)\n",
       "  )\n",
       "  (Decoder): Decoder(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (attention): Attention(\n",
       "      (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(7853, 512)\n",
       "    (LSTM): LSTM(1536, 512, num_layers=2)\n",
       "    (fc): Linear(in_features=2048, out_features=7853, bias=True)\n",
       "  )\n",
       "  (target_vocab): Vocab()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_params[\"input_size_encoder\"] = len(nmtds_train.src_vocab)\n",
    "hyp_params[\"input_size_decoder\"] = len(nmtds_train.trg_vocab)\n",
    "hyp_params[\"output_size\"] = len(nmtds_train.trg_vocab)\n",
    "\n",
    "\n",
    "model_l = SeqtoSeq(hyp_params, nmtds_train.trg_vocab, pad_idx, device=device)\n",
    "model_l.load_state_dict(torch.load('model-attention.pt', map_location=device)[\"model_state_dict\"])\n",
    "model_l.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffb2c650-6614-4034-9b4a-12b828bb6a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:31<00:00, 31.58it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26772984862327576"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.bleu(model_l, nmtds_test, True, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd67f4b-a02e-47e5-9abb-19a3ac2e19d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=15)\n",
    "    \n",
    "    x_ticks = [''] + ['<sos>'] + [t.lower() for t in sentence] + ['<eos>']\n",
    "    y_ticks = [''] + translation\n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def translate(tokens, dataset, model, device):\n",
    "    indices = [dataset.src_vocab['<sos>']] + dataset.src_vocab.lookup_indices(tokens) + [dataset.src_vocab['<eos>']]\n",
    "    inp_tensor = torch.tensor(indices).unsqueeze(1).to(device)\n",
    "\n",
    "    # Build encoder hidden, cell state\n",
    "    with torch.no_grad():\n",
    "        eouts, hidden, cell = model.Encoder(inp_tensor)\n",
    "\n",
    "    outputs = [dataset.trg_vocab[\"<sos>\"]]\n",
    "    mask = model.create_mask(inp_tensor)\n",
    "    attentions = torch.zeros(50, 1, len(indices)).to(device)\n",
    "\n",
    "    for i in range(50):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell, attention = model.Decoder(previous_word, eouts, hidden, cell, mask)\n",
    "            attentions[i] = attention\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # M odel predicts it's the end of the sentence\n",
    "        if output.argmax(1).item() == dataset.trg_vocab['<eos>']:\n",
    "            break\n",
    "\n",
    "    trg_tokens = dataset.trg_vocab.lookup_tokens(outputs)\n",
    "    return trg_tokens[1:], attentions[:len(trg_tokens)-1]\n",
    "\n",
    "src = 'Two young, White males are outside near many bushes'\n",
    "src_tokens = nmtds_train.tokenizers['en'](src.lower().strip())\n",
    "translation, attention = translate(src_tokens, nmtds_train, model_l, device)\n",
    "translation\n",
    "display_attention(src_tokens, translation, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675db793-efbe-44c5-8115-76c600aefed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
