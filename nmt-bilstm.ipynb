{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e62732-00db-4b4e-8a3e-69be08de0fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\father\\.conda\\envs\\maq\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import nmtDataset\n",
    "import helpers as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58b39d2b-bbf2-4235-ad90-af8ee6d59ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75d3b3cc-db53-4283-8baa-0b4b4ad18924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding for consistency in reproducibility\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea59b269-3071-4785-9651-fe811872aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params = {\n",
    "    \"batch_size\": 128,\n",
    "    \"num_epochs\": 10,\n",
    "\n",
    "    # Encoder parameters\n",
    "    \"encoder_embedding_size\": 512,\n",
    "    \"encoder_dropout\": 0.5,\n",
    "\n",
    "    # Decoder parameters\n",
    "    \"decoder_dropout\": 0.5,\n",
    "    \"decoder_embedding_size\": 512,\n",
    "\n",
    "    # Common parameters\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layers\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7daf9a0c-37b3-4dad-9d85-0c2ad4bdd89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = utils.Logger('logs/emd512-enc2-dec2-bilstm.out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116d1f7f-0055-49e5-864f-84797b2484e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (61.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.22.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.3.0/de_core_news_sm-3.3.0-py3-none-any.whl (14.6 MB)\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from de-core-news-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.22.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (4.64.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.27.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (61.2.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2022.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\father\\.conda\\envs\\maq\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->de-core-news-sm==3.3.0) (2.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77f80e9c-9731-4b1c-b4f3-639e93958911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_size, num_layers, dropout=dropout, bidirectional = True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Shape (embedding) --> [Sequence_length , batch_size , embedding dims]\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # ************** Multiplied by 2 because of bi-directional LSTM\n",
    "        # Shape --> (output) [Sequence_length , batch_size , hidden_size * 2]\n",
    "        # Shape --> (hs, cs) [num_layers * 2, batch_size size, hidden_size]\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "        \n",
    "        return hidden_state, cell_state\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "         # ************** Multiplying 2 because of bi-directional LSTM\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_size, num_layers * 2, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        # As we are not feeding whole sentence we will each token a time\n",
    "        # hence our sequence length would be just 1 however shape of x is batch_size\n",
    "        # to add sequence length we will unsequeeze it\n",
    "        # Shape (x) --> [batch_size] (see seq2seq model) so making it [1, batch_size]\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        # Shape (embedded) --> (1, batch_size, embedding dims)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # Shape (outputs) --> (1, 32, 1024) [1, batch_size , hidden_size]\n",
    "        # Shape (hs, cl) --> (2, 32, 1024)  [num_layers * 2, batch_size , hidden_size]\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedded, (hidden_state, cell_state))\n",
    "\n",
    "        # Shape (outputs) -->  (1, batch_size, hidden_size)\n",
    "        # Shape (outputs.squeeze(0)) -->  (batch_size, hidden_size)\n",
    "        # Shape (predictions) --> (batch_size, target_vocab_size)\n",
    "        predictions = self.fc(outputs.squeeze(0))\n",
    "        \n",
    "        return predictions, hidden_state, cell_state\n",
    "\n",
    "class SeqtoSeq(nn.Module):\n",
    "    def __init__(self, gen_params, target_vocab, device):\n",
    "        super(SeqtoSeq, self).__init__()\n",
    "\n",
    "        self.Encoder = Encoder(gen_params[\"input_size_encoder\"],\n",
    "                          gen_params[\"encoder_embedding_size\"],\n",
    "                          gen_params[\"hidden_size\"],\n",
    "                          gen_params[\"num_layers\"],\n",
    "                          gen_params[\"encoder_dropout\"]).to(device)\n",
    "\n",
    "        self.Decoder = Decoder(gen_params[\"input_size_decoder\"],\n",
    "                          gen_params[\"decoder_embedding_size\"],\n",
    "                          gen_params[\"hidden_size\"],\n",
    "                          gen_params[\"num_layers\"],\n",
    "                          gen_params[\"decoder_dropout\"],\n",
    "                          gen_params[\"output_size\"]).to(device)\n",
    "\n",
    "        self.target_vocab = target_vocab\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, source, src_lens, target, tfr=0.5):\n",
    "        # Shape -> (Sentence length, Batch_size)\n",
    "        batch_size = source.shape[1]\n",
    "\n",
    "        target_len = target.shape[0]  # Length of target sentences\n",
    "        target_vocab_size = len(self.target_vocab)\n",
    "        \n",
    "        # here we will store all the outputs\n",
    "        # so outputs is arrange in a way that sentences are in column and batch size is row and every element\n",
    "        # will consist of probability of each word from the vocab\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
    "\n",
    "        # Shape --> (hs, cs) (num_layers * 2, batch_size size, hidden_size) (contains encoder's hs, cs - context vectors)\n",
    "        hidden_state, cell_state = self.Encoder(source)\n",
    "\n",
    "        # Shape (target) -> (Sentence length, Batch_size)\n",
    "        # Shape (x) --> (batch_size)\n",
    "        x = target[0]  # First token (Trigger)\n",
    "        \n",
    "        for i in range(1, target_len):\n",
    "            # Shape (output) --> (batch_size, target_vocab_size)\n",
    "            # Shape (hs, cl) --> (num_layers * 2, batch_size , hidden_size)\n",
    "            output, hidden_state, cell_state = self.Decoder(x, hidden_state, cell_state)\n",
    "            outputs[i] = output\n",
    "            best_guess = output.argmax(1)  # 0th dimension is batch size, 1st dimension is word embedding\n",
    "            # Schedule sampling\n",
    "            x = target[\n",
    "                i] if random.random() < tfr else best_guess  # Either pass the next word correctly from the dataset\n",
    "            # or use the earlier predicted word\n",
    "\n",
    "        # Shape --> (sentence length, batch size, vocab size)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "772e5645-da1e-4ac6-9bd1-ab3e2ffb13f0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "nmtds_train = nmtDataset('datasets/Multi30k/', 'train')\n",
    "nmtds_valid = nmtDataset('datasets/Multi30k/', 'val', nmtds_train)\n",
    "nmtds_test = nmtDataset('datasets/Multi30k/', 'test', nmtds_train)\n",
    "\n",
    "pad_idx = nmtds_train.trg_vocab[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73779a22-9baa-49d5-a298-13b1abe6d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(nmtds_train, batch_size=hyp_params['batch_size'], shuffle=True,\n",
    "                              collate_fn=lambda batch_size: utils.collate_fn(batch_size, pad_idx, device))\n",
    "\n",
    "valid_dataloader = DataLoader(nmtds_valid, batch_size=hyp_params['batch_size'], shuffle=True,\n",
    "                              collate_fn=lambda batch_size: utils.collate_fn(batch_size, pad_idx, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe2e61ed-c875-4a9a-9c57-60a9b82ee18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params[\"input_size_encoder\"] = len(nmtds_train.src_vocab)\n",
    "hyp_params[\"input_size_decoder\"] = len(nmtds_train.trg_vocab)\n",
    "hyp_params[\"output_size\"] = len(nmtds_train.trg_vocab)\n",
    "\n",
    "model = SeqtoSeq(hyp_params, target_vocab=nmtds_train.trg_vocab, device=device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e89c4c12-8ad2-45cc-8d38-38a8e0af9e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 227/227 [00:37<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 5.234851837158203, Eval loss: 4.6997880935668945, patience: 1. Time 38.24520540237427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 227/227 [00:36<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 4.6150126457214355, Eval loss: 4.345252513885498, patience: 1. Time 36.634130239486694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 227/227 [00:35<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 4.262725830078125, Eval loss: 4.234218597412109, patience: 1. Time 36.38913941383362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 227/227 [00:35<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train loss: 3.9060938358306885, Eval loss: 4.087944984436035, patience: 1. Time 36.37311601638794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 227/227 [00:36<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 3.684907913208008, Eval loss: 3.9035472869873047, patience: 1. Time 36.553874015808105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 227/227 [00:36<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train loss: 3.521395683288574, Eval loss: 3.8680381774902344, patience: 1. Time 36.788660526275635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 227/227 [00:35<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train loss: 3.381364583969116, Eval loss: 3.8179566860198975, patience: 1. Time 36.15723180770874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 227/227 [00:35<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train loss: 3.2475948333740234, Eval loss: 3.800525188446045, patience: 1. Time 36.21805024147034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 227/227 [00:35<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train loss: 3.146925926208496, Eval loss: 3.7838470935821533, patience: 1. Time 36.34213995933533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 227/227 [00:35<00:00,  6.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 3.0509862899780273, Eval loss: 3.79226016998291, patience: 1. Time 36.379178285598755\n"
     ]
    }
   ],
   "source": [
    "min_el = math.inf\n",
    "patience = 1\n",
    "best_model = {}\n",
    "best_epoch = 0\n",
    "\n",
    "epoch_loss = 0\n",
    "for epoch in range(hyp_params[\"num_epochs\"]):\n",
    "    start = time.time()\n",
    "    \n",
    "    epoch_loss = utils.train_model(model, train_dataloader, criterion, optimizer)\n",
    "    eval_loss = utils.evaluate_model(model, valid_dataloader, criterion)\n",
    "    \n",
    "    log.log(f\"Epoch: {epoch+1}, Train loss: {epoch_loss}, Eval loss: {eval_loss}, patience: {patience}. Time {time.time() - start}\")\n",
    "\n",
    "    \n",
    "    if eval_loss < min_el:\n",
    "        best_epoch = epoch+1\n",
    "        min_el = eval_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'eval_loss': min_el\n",
    "        }, 'model-bilstm.pt')\n",
    "        patience = 1\n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    if patience == 10:\n",
    "        log.log(\"[STOPPING] Early stopping in action..\")\n",
    "        log.log(f\"Best epoch was {best_epoch} with {min_el} eval loss\")\n",
    "        break\n",
    "\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86a2bd3d-1eac-41c5-a4f3-57deb26f8237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqtoSeq(\n",
       "  (Encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(5893, 512)\n",
       "    (LSTM): LSTM(512, 512, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  )\n",
       "  (Decoder): Decoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(7853, 512)\n",
       "    (LSTM): LSTM(512, 512, num_layers=4, dropout=0.5)\n",
       "    (fc): Linear(in_features=512, out_features=7853, bias=True)\n",
       "  )\n",
       "  (target_vocab): Vocab()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_l = SeqtoSeq(hyp_params, target_vocab=nmtds_train.trg_vocab, device=device)\n",
    "model_l.load_state_dict(torch.load('model-bilstm.pt', map_location=device)[\"model_state_dict\"])\n",
    "model_l.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffb2c650-6614-4034-9b4a-12b828bb6a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:08<00:00, 113.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2086530178785324"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.bleu(model_l, nmtds_test, False, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a8fcfe-0047-4fc3-a6ed-fcc5a4d5d2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
