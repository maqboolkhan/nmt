{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "93ca8d46-0e2e-4d9b-b9a7-de35eb0763cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Transformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchtext\n",
    "\n",
    "import gc\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "from dataset import nmtDataset\n",
    "import helpers as utils\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e93a05a-37ec-49cf-9fd6-8920dd074953",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b016e667-484b-41f7-89b4-4e3f8b8da896",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = utils.Logger('logs/transformers.out')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13d5143e-4bb3-4fe1-8b7d-cc0fdf976d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params = {\n",
    "    \"batch_size\": 128,\n",
    "    \"num_epochs\": 10,\n",
    "    \"d_model\": 512,\n",
    "    \"n_head\": 8,\n",
    "    \"num_encoder_layers\": 3,\n",
    "    \"num_decoder_layers\": 3,\n",
    "    \"feedforward_dim\": 128,\n",
    "    \"dropout\": 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3407d825-9a5f-4b71-a315-825b17256f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmtds_train = nmtDataset('datasets/Multi30k/', 'train')\n",
    "nmtds_valid = nmtDataset('datasets/Multi30k/', 'val', nmtds_train)\n",
    "nmtds_test = nmtDataset('datasets/Multi30k/', 'test', nmtds_train)\n",
    "\n",
    "SRC_PAD_IDX = nmtds_train.src_vocab[\"<pad>\"]\n",
    "TRG_PAD_IDX = nmtds_train.trg_vocab[\"<pad>\"]\n",
    "\n",
    "train_dataloader = DataLoader(nmtds_train, batch_size=hyp_params[\"batch_size\"], shuffle=True,\n",
    "                              collate_fn=lambda batch_size: utils.collate_fn(batch_size, SRC_PAD_IDX, device))\n",
    "\n",
    "valid_dataloader = DataLoader(nmtds_valid, batch_size=hyp_params[\"batch_size\"], shuffle=True,\n",
    "                              collate_fn=lambda batch_size: utils.collate_fn(batch_size, SRC_PAD_IDX, device))\n",
    "\n",
    "hyp_params[\"src_vocab_size\"] = len(nmtds_train.src_vocab)\n",
    "hyp_params[\"trg_vocab_size\"] = len(nmtds_train.trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e79d461-7517-46a9-95db-581de366e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, maxlen = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # A tensor consists of all the possible positions (index) e.g 0, 1, 2, ... max length of input\n",
    "        # Shape (pos) --> [max len, 1]\n",
    "        pos = torch.arange(0, maxlen).unsqueeze(1)\n",
    "        \n",
    "        pos_encoding = torch.zeros((maxlen, d_model))\n",
    "        \n",
    "        # In the paper, they had 2i in the positional encoding formula\n",
    "        # where i is the dimension \n",
    "        sin_den = 10000 ** (torch.arange(0, d_model, 2)/d_model) # sin for even item of position's dimension\n",
    "        cos_den = 10000 ** (torch.arange(1, d_model, 2)/d_model) # cos for odd \n",
    "        \n",
    "        pos_encoding[:, 0::2] = torch.sin(pos / sin_den) \n",
    "        pos_encoding[:, 1::2] = torch.cos(pos / cos_den)\n",
    "        \n",
    "        # Shape (pos_embedding) --> [max len, d_model]\n",
    "        pos_encoding = pos_encoding.unsqueeze(-2)\n",
    "        # Shape (pos_embedding) --> [max len, 1, d_model]\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        \n",
    "        # shape (token_embedding) --> [sentence len, batch size, d_model]\n",
    "        \n",
    "        # Combining embeddings with positional encodings\n",
    "        # Note: As we made positional encoding with the size max length of sentence in our dataset \n",
    "        #       hence here we are picking till the sentence length in a batch\n",
    "        #       Another thing to notice is in the paper they used FIXED positional encoding, there are\n",
    "        #       methods where we can also learn them but we are doing as presented in the paper\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "    \n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # shape (tokens) --> [sentence len, batch size]\n",
    "        # shape (inp_emb) --> [sentence len, batch size, d_model]\n",
    "        inp_emb = self.embedding(tokens.long()) * math.sqrt(self.d_model)\n",
    "        return inp_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1cba2350-fb0b-4f6b-a41a-591d03cdca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 src_vocab_size, \n",
    "                 trg_vocab_size, \n",
    "                 d_model, \n",
    "                 dropout,\n",
    "                 nhead,\n",
    "                 num_encoder_layers,\n",
    "                 num_decoder_layers,\n",
    "                 dim_feedforward,\n",
    "                 src_pad_idx,\n",
    "                 trg_pad_idx\n",
    "                ):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        \n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "        self.src_inp_emb = InputEmbedding(src_vocab_size, d_model)\n",
    "        self.trg_inp_emb = InputEmbedding(trg_vocab_size, d_model)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        self.transformer = Transformer(d_model=d_model,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        \n",
    "        self.generator = nn.Linear(d_model, trg_vocab_size)\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        src_emb = self.positional_encoding(self.src_inp_emb(src))\n",
    "        trg_emb = self.positional_encoding(self.trg_inp_emb(trg))\n",
    "        \n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, trg)\n",
    "        \n",
    "        outs = self.transformer(src = src_emb, \n",
    "                                tgt = trg_emb, \n",
    "                                src_mask = src_mask,\n",
    "                                tgt_mask = tgt_mask, \n",
    "                                src_key_padding_mask = src_padding_mask, \n",
    "                                tgt_key_padding_mask = tgt_padding_mask)\n",
    "        return self.generator(outs)\n",
    "        \n",
    "\n",
    "    def create_mask(self, src, tgt):\n",
    "        src_seq_len = src.shape[0]\n",
    "        tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_seq_len)\n",
    "        src_mask = torch.zeros((src_seq_len, src_seq_len),device=device).type(torch.bool) # All False hence unchanged\n",
    "\n",
    "        # Padding masking will allow attention to ignore padding <pad> tokens \n",
    "        src_padding_mask = (src == self.src_pad_idx).transpose(0, 1)\n",
    "        tgt_padding_mask = (tgt == self.trg_pad_idx).transpose(0, 1)\n",
    "        \n",
    "        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2993bac6-a5c2-47fe-b190-4f39953fe817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # shape (src, trg) --> [seq len, batch size]\n",
    "        src = batch[\"src\"]\n",
    "        trg = batch[\"trg\"]\n",
    "\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # shape (trg_inp, trg_out) --> [seq len - 1, batch size]\n",
    "        trg_inp = trg[:-1, :]\n",
    "        trg_out = trg[1:, :]\n",
    "\n",
    "        # shape --> (seq len - 1) * batch size \n",
    "        # Making all target seqeunces in 1d tensor\n",
    "        trg_out = trg_out.reshape(-1)\n",
    "\n",
    "        # shape (logits) --> [seq len - 1, batch size, trg vocab size]\n",
    "        logits = transformer(src, trg_inp)\n",
    "\n",
    "        # shape (logits) --> [(seq len - 1) * batch size, trg vocab size]\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "\n",
    "        loss = criterion(logits, trg_out)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().cpu()\n",
    "\n",
    "    return epoch_loss/len(train_dataloader)\n",
    "\n",
    "def evaluate_model(transformer, valid_dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(valid_dataloader):\n",
    "            # shape (src, trg) --> [seq len, batch size]\n",
    "            src = batch[\"src\"]\n",
    "            trg = batch[\"trg\"]\n",
    "\n",
    "            # shape (trg_inp, trg_out) --> [seq len - 1, batch size]\n",
    "            trg_inp = trg[:-1, :]\n",
    "            trg_out = trg[1:, :]\n",
    "\n",
    "            # shape --> (seq len - 1) * batch size \n",
    "            # Making all target seqeunces in 1d tensor\n",
    "            trg_out = trg_out.reshape(-1)\n",
    "\n",
    "            # shape (logits) --> [seq len - 1, batch size, trg vocab size]\n",
    "            logits = transformer(src, trg_inp)\n",
    "\n",
    "            # shape (logits) --> [(seq len - 1) * batch size, trg vocab size]\n",
    "            logits = logits.reshape(-1, logits.shape[-1])\n",
    "\n",
    "            loss = criterion(logits, trg_out)\n",
    "\n",
    "            epoch_loss += loss.detach().cpu()\n",
    "    \n",
    "    return epoch_loss/len(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "83a78a5d-596b-4672-9b84-75596fd6e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(hyp_params[\"src_vocab_size\"],\n",
    "                                hyp_params[\"trg_vocab_size\"],\n",
    "                                hyp_params[\"d_model\"],\n",
    "                                hyp_params[\"dropout\"],\n",
    "                                hyp_params[\"n_head\"],\n",
    "                                hyp_params[\"num_encoder_layers\"],\n",
    "                                hyp_params[\"num_decoder_layers\"],\n",
    "                                hyp_params[\"feedforward_dim\"],\n",
    "                                SRC_PAD_IDX,\n",
    "                                TRG_PAD_IDX\n",
    "                                ).to(device)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd1a932-6579-4272-9a23-dceae964232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_el = math.inf\n",
    "patience = 1\n",
    "best_model = {}\n",
    "best_epoch = 0\n",
    "\n",
    "epoch_loss = 0\n",
    "for epoch in range(hyp_params[\"num_epochs\"]):\n",
    "    start = time.time()\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    epoch_loss = train_model(transformer, train_dataloader, criterion, optimizer)\n",
    "    eval_loss = evaluate_model(transformer, valid_dataloader, criterion)\n",
    "    \n",
    "    log.log(f\"Epoch: {epoch+1}, Train loss: {epoch_loss}, Eval loss: {eval_loss}, patience: {patience}. Time {time.time() - start}\")\n",
    "\n",
    "    \n",
    "    if eval_loss < min_el:\n",
    "        best_epoch = epoch\n",
    "        min_el = eval_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'eval_loss': min_el\n",
    "        }, 'model-attention-mask.pt')\n",
    "        patience = 1\n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    if patience == 10:\n",
    "        log.log(\"[STOPPING] Early stopping in action..\")\n",
    "        log.log(f\"Best epoch was {best_epoch} with {min_el} eval loss\")\n",
    "        break\n",
    "        \n",
    "log.log(f\"Best epoch was {best_epoch} with {min_el} eval loss\")\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3bac57-24ba-46bc-9075-f8017e195243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
