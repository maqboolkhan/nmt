{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93ca8d46-0e2e-4d9b-b9a7-de35eb0763cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Transformer\n",
    "from tqdm import tqdm\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "import torchtext\n",
    "\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "from dataset import nmtDataset\n",
    "import helpers as utils\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e93a05a-37ec-49cf-9fd6-8920dd074953",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b016e667-484b-41f7-89b4-4e3f8b8da896",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = utils.Logger('logs/transformers.out')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13d5143e-4bb3-4fe1-8b7d-cc0fdf976d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_params = {\n",
    "    \"batch_size\": 128,\n",
    "    \"lr\": 0.0005,\n",
    "    \"num_epochs\": 10,\n",
    "    \"d_model\": 512,\n",
    "    \"n_head\": 8,\n",
    "    \"num_encoder_layers\": 3,\n",
    "    \"num_decoder_layers\": 3,\n",
    "    \"feedforward_dim\": 256,\n",
    "    \"dropout\": 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3407d825-9a5f-4b71-a315-825b17256f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmtds_train = nmtDataset('datasets/Multi30k/', 'train')\n",
    "nmtds_valid = nmtDataset('datasets/Multi30k/', 'val', nmtds_train)\n",
    "nmtds_test = nmtDataset('datasets/Multi30k/', 'test', nmtds_train)\n",
    "\n",
    "SRC_PAD_IDX = nmtds_train.src_vocab[\"<pad>\"]\n",
    "TRG_PAD_IDX = nmtds_train.trg_vocab[\"<pad>\"]\n",
    "\n",
    "train_dataloader = DataLoader(nmtds_train, batch_size=hyp_params[\"batch_size\"], shuffle=True,\n",
    "                              collate_fn=lambda batch_size: utils.collate_fn(batch_size, SRC_PAD_IDX, device))\n",
    "\n",
    "valid_dataloader = DataLoader(nmtds_valid, batch_size=hyp_params[\"batch_size\"], shuffle=True,\n",
    "                              collate_fn=lambda batch_size: utils.collate_fn(batch_size, SRC_PAD_IDX, device))\n",
    "\n",
    "hyp_params[\"src_vocab_size\"] = len(nmtds_train.src_vocab)\n",
    "hyp_params[\"trg_vocab_size\"] = len(nmtds_train.trg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e79d461-7517-46a9-95db-581de366e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, maxlen = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # A tensor consists of all the possible positions (index) e.g 0, 1, 2, ... max length of input\n",
    "        # Shape (pos) --> [max len, 1]\n",
    "        pos = torch.arange(0, maxlen).unsqueeze(1)\n",
    "        \n",
    "        pos_encoding = torch.zeros((maxlen, d_model))\n",
    "        \n",
    "        # In the paper, they had 2i in the positional encoding formula\n",
    "        # where i is the dimension \n",
    "        sin_den = 10000 ** (torch.arange(0, d_model, 2)/d_model) # sin for even item of position's dimension\n",
    "        cos_den = 10000 ** (torch.arange(1, d_model, 2)/d_model) # cos for odd \n",
    "        \n",
    "        pos_encoding[:, 0::2] = torch.sin(pos / sin_den) \n",
    "        pos_encoding[:, 1::2] = torch.cos(pos / cos_den)\n",
    "        \n",
    "        # Shape (pos_embedding) --> [max len, d_model]\n",
    "        pos_encoding = pos_encoding.unsqueeze(-2)\n",
    "        # Shape (pos_embedding) --> [max len, 1, d_model]\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        \n",
    "        # shape (token_embedding) --> [sentence len, batch size, d_model]\n",
    "        \n",
    "        # Combining embeddings with positional encodings\n",
    "        # Note: As we made positional encoding with the size max length of sentence in our dataset \n",
    "        #       hence here we are picking till the sentence length in a batch\n",
    "        #       Another thing to notice is in the paper they used FIXED positional encoding, there are\n",
    "        #       methods where we can also learn them but we are doing as presented in the paper\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "    \n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # shape (tokens) --> [sentence len, batch size]\n",
    "        # shape (inp_emb) --> [sentence len, batch size, d_model]\n",
    "        inp_emb = self.embedding(tokens.long()) * math.sqrt(self.d_model)\n",
    "        return inp_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cba2350-fb0b-4f6b-a41a-591d03cdca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 src_vocab_size, \n",
    "                 trg_vocab_size, \n",
    "                 d_model, \n",
    "                 dropout,\n",
    "                 nhead,\n",
    "                 num_encoder_layers,\n",
    "                 num_decoder_layers,\n",
    "                 dim_feedforward,\n",
    "                 src_pad_idx,\n",
    "                 trg_pad_idx\n",
    "                ):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        \n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "        self.src_inp_emb = InputEmbedding(src_vocab_size, d_model)\n",
    "        self.trg_inp_emb = InputEmbedding(trg_vocab_size, d_model)\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        self.transformer = Transformer(d_model=d_model,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        \n",
    "        self.generator = nn.Linear(d_model, trg_vocab_size)\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        src_emb = self.positional_encoding(self.src_inp_emb(src))\n",
    "        trg_emb = self.positional_encoding(self.trg_inp_emb(trg))\n",
    "        \n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, trg)\n",
    "        \n",
    "        outs = self.transformer(src = src_emb, \n",
    "                                tgt = trg_emb, \n",
    "                                src_mask = src_mask,\n",
    "                                tgt_mask = tgt_mask, \n",
    "                                src_key_padding_mask = src_padding_mask, \n",
    "                                tgt_key_padding_mask = tgt_padding_mask,\n",
    "                                memory_key_padding_mask = src_padding_mask\n",
    "                               )\n",
    "        return self.generator(outs)\n",
    "        \n",
    "\n",
    "    def create_mask(self, src, trg):\n",
    "        src_seq_len = src.shape[0]\n",
    "        trg_seq_len = trg.shape[0]\n",
    "\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_len).to(device)\n",
    "        src_mask = torch.zeros((src_seq_len, src_seq_len),device=device).type(torch.bool) # All False hence unchanged\n",
    "\n",
    "        # Padding masking will allow attention to ignore padding <pad> tokens \n",
    "        src_padding_mask = (src == self.src_pad_idx).transpose(0, 1)\n",
    "        trg_padding_mask = (trg == self.trg_pad_idx).transpose(0, 1)\n",
    "        \n",
    "        return src_mask, trg_mask, src_padding_mask, trg_padding_mask\n",
    "    \n",
    "    def encode(self, src):\n",
    "        src_mask = torch.zeros((src.shape[0], src.shape[0]),device=device).type(torch.bool)\n",
    "        src_padding_mask = (src == self.src_pad_idx).transpose(0, 1)\n",
    "        \n",
    "        return self.transformer.encoder(self.positional_encoding(self.src_inp_emb(src)), src_mask, src_padding_mask)\n",
    "\n",
    "    def decode(self, trg, memory):\n",
    "        trg_seq_len = trg.shape[0]\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_len).type(torch.bool).to(device)\n",
    "        trg_padding_mask = (trg == self.trg_pad_idx).transpose(0, 1)\n",
    "        \n",
    "        return self.transformer.decoder(tgt = self.positional_encoding(self.trg_inp_emb(trg)), \n",
    "                                        memory = memory, \n",
    "                                        tgt_mask = trg_mask,\n",
    "                                        tgt_key_padding_mask = trg_padding_mask\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2993bac6-a5c2-47fe-b190-4f39953fe817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # shape (src, trg) --> [seq len, batch size]\n",
    "        src = batch[\"src\"]\n",
    "        trg = batch[\"trg\"]\n",
    "\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # shape (trg_inp, trg_out) --> [seq len - 1, batch size]\n",
    "        trg_inp = trg[:-1, :]\n",
    "        trg_out = trg[1:, :]\n",
    "\n",
    "        # shape --> (seq len - 1) * batch size \n",
    "        # Making all target seqeunces in 1d tensor\n",
    "        trg_out = trg_out.reshape(-1)\n",
    "\n",
    "        # shape (logits) --> [seq len - 1, batch size, trg vocab size]\n",
    "        logits = model(src, trg_inp)\n",
    "\n",
    "        # shape (logits) --> [(seq len - 1) * batch size, trg vocab size]\n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "\n",
    "        loss = criterion(logits, trg_out)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().cpu()\n",
    "\n",
    "    return epoch_loss/len(train_dataloader)\n",
    "\n",
    "def evaluate_model(model, valid_dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(valid_dataloader):\n",
    "            # shape (src, trg) --> [seq len, batch size]\n",
    "            src = batch[\"src\"]\n",
    "            trg = batch[\"trg\"]\n",
    "\n",
    "            # shape (trg_inp, trg_out) --> [seq len - 1, batch size]\n",
    "            trg_inp = trg[:-1, :]\n",
    "            trg_out = trg[1:, :]\n",
    "\n",
    "            # shape --> (seq len - 1) * batch size \n",
    "            # Making all target seqeunces in 1d tensor\n",
    "            trg_out = trg_out.reshape(-1)\n",
    "\n",
    "            # shape (logits) --> [seq len - 1, batch size, trg vocab size]\n",
    "            logits = model(src, trg_inp)\n",
    "\n",
    "            # shape (logits) --> [(seq len - 1) * batch size, trg vocab size]\n",
    "            logits = logits.reshape(-1, logits.shape[-1])\n",
    "\n",
    "            loss = criterion(logits, trg_out)\n",
    "\n",
    "            epoch_loss += loss.detach().cpu()\n",
    "    \n",
    "    return epoch_loss/len(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83a78a5d-596b-4672-9b84-75596fd6e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqTransformer(hyp_params[\"src_vocab_size\"],\n",
    "                                hyp_params[\"trg_vocab_size\"],\n",
    "                                hyp_params[\"d_model\"],\n",
    "                                hyp_params[\"dropout\"],\n",
    "                                hyp_params[\"n_head\"],\n",
    "                                hyp_params[\"num_encoder_layers\"],\n",
    "                                hyp_params[\"num_decoder_layers\"],\n",
    "                                hyp_params[\"feedforward_dim\"],\n",
    "                                SRC_PAD_IDX,\n",
    "                                TRG_PAD_IDX\n",
    "                                ).to(device)\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyp_params[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6dd1a932-6579-4272-9a23-dceae964232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:15<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 4.186300277709961, Eval loss: 2.948056697845459, patience: 1. Time 76.640953540802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 2.5421230792999268, Eval loss: 2.004237651824951, patience: 1. Time 76.4445264339447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 1.8266888856887817, Eval loss: 1.6303128004074097, patience: 1. Time 76.26595902442932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:16<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train loss: 1.4357585906982422, Eval loss: 1.482712984085083, patience: 1. Time 77.28322458267212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:16<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 1.176565408706665, Eval loss: 1.4040013551712036, patience: 1. Time 77.23536419868469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:16<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train loss: 0.9969968199729919, Eval loss: 1.373586654663086, patience: 1. Time 77.3201367855072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train loss: 0.85317462682724, Eval loss: 1.3977683782577515, patience: 1. Time 76.52227067947388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:15<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train loss: 0.7432900071144104, Eval loss: 1.4157357215881348, patience: 2. Time 76.57517194747925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:15<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train loss: 0.6643882393836975, Eval loss: 1.4319831132888794, patience: 3. Time 76.32783889770508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 227/227 [01:15<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 0.5918142199516296, Eval loss: 1.4792366027832031, patience: 4. Time 76.65888452529907\n",
      "Best epoch was 6 with 1.373586654663086 eval loss\n"
     ]
    }
   ],
   "source": [
    "min_el = math.inf\n",
    "patience = 1\n",
    "best_model = {}\n",
    "best_epoch = 0\n",
    "\n",
    "epoch_loss = 0\n",
    "for epoch in range(hyp_params[\"num_epochs\"]):\n",
    "    start = time.time()\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    epoch_loss = train_model(model, train_dataloader, criterion, optimizer)\n",
    "    eval_loss = evaluate_model(model, valid_dataloader, criterion)\n",
    "    \n",
    "    log.log(f\"Epoch: {epoch+1}, Train loss: {epoch_loss}, Eval loss: {eval_loss}, patience: {patience}. Time {time.time() - start}\")\n",
    "\n",
    "    \n",
    "    if eval_loss < min_el:\n",
    "        best_epoch = epoch+1\n",
    "        min_el = eval_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'eval_loss': min_el\n",
    "        }, 'model-transformer.pt')\n",
    "        patience = 1\n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    if patience == 10:\n",
    "        log.log(\"[STOPPING] Early stopping in action..\")\n",
    "        log.log(f\"Best epoch was {best_epoch} with {min_el} eval loss\")\n",
    "        break\n",
    "        \n",
    "log.log(f\"Best epoch was {best_epoch} with {min_el} eval loss\")\n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c3bac57-24ba-46bc-9075-f8017e195243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (src_inp_emb): InputEmbedding(\n",
       "    (embedding): Embedding(5893, 512)\n",
       "  )\n",
       "  (trg_inp_emb): InputEmbedding(\n",
       "    (embedding): Embedding(7853, 512)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (generator): Linear(in_features=512, out_features=7853, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_l = Seq2SeqTransformer(hyp_params[\"src_vocab_size\"],\n",
    "                                hyp_params[\"trg_vocab_size\"],\n",
    "                                hyp_params[\"d_model\"],\n",
    "                                hyp_params[\"dropout\"],\n",
    "                                hyp_params[\"n_head\"],\n",
    "                                hyp_params[\"num_encoder_layers\"],\n",
    "                                hyp_params[\"num_decoder_layers\"],\n",
    "                                hyp_params[\"feedforward_dim\"],\n",
    "                                SRC_PAD_IDX,\n",
    "                                TRG_PAD_IDX\n",
    "                                ).to(device)\n",
    "\n",
    "model_l.load_state_dict(torch.load('model-transformer.pt', map_location=device)[\"model_state_dict\"])\n",
    "model_l.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6cf7be70-bda6-45bc-a179-1cb9f40f66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(snt, dataset, model, device):\n",
    "    snt = torch.tensor(snt).view(-1,1).to(device)\n",
    "    \n",
    "    num_tokens = snt.shape[0]\n",
    "    max_len = 50\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        memory = model.encode(snt).to(device)\n",
    "    \n",
    "    ys = torch.LongTensor([dataset.trg_vocab['<sos>']]).unsqueeze(0).to(device)\n",
    "\n",
    "    for i in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            out = model.decode(ys, memory)\n",
    "            \n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        next_word = prob.argmax().detach().item()\n",
    "\n",
    "        ys = torch.cat([ys, torch.tensor([next_word]).unsqueeze(1).to(device)], dim=0)\n",
    "        \n",
    "        \n",
    "        if next_word == dataset.trg_vocab['<eos>']:\n",
    "            break\n",
    "    \n",
    "    return dataset.trg_vocab.lookup_tokens(ys.squeeze().cpu().numpy())\n",
    "\n",
    "\n",
    "def bleu(model, dataset, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "\n",
    "    for example in tqdm(dataset):\n",
    "        src = example[\"src\"]\n",
    "        trg = example[\"trg\"]\n",
    "        \n",
    "        trg = dataset.trg_vocab.lookup_tokens(trg)    \n",
    "        prediction = translate(src, dataset, model, device)\n",
    "        \n",
    "        prediction = prediction[1:-1]  # removing <sos> and <eos> tokens\n",
    "        trg = trg[1:-1]\n",
    "        \n",
    "        targets.append([trg])\n",
    "        outputs.append(prediction)\n",
    "\n",
    "    return bleu_score(outputs, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff28e153-7e78-4864-88fd-759fe22a3fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:57<00:00, 17.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3632104694843292"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu(model_l, nmtds_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f68e85-5d3d-4706-8039-e0989622360e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
